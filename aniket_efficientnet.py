# -*- coding: utf-8 -*-
"""Aniket_EfficientNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1buFAJuvwJGQ0_vg5z5fy_Dni8UA9dMHD
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import Libraries
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
import os
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Step 3: Set Dataset Paths
main_folder = '/content/drive/MyDrive/LDC/data'
train_dir = os.path.join(main_folder, 'train')
val_dir = os.path.join(main_folder, 'valid')
test_dir = os.path.join(main_folder, 'test')

# Step 4: Load the Datasets
batch_size = 32
img_size = (224, 224)

train_ds = image_dataset_from_directory(train_dir, image_size=img_size, batch_size=batch_size, shuffle=True)
val_ds = image_dataset_from_directory(val_dir, image_size=img_size, batch_size=batch_size, shuffle=False)
test_ds = image_dataset_from_directory(test_dir, image_size=img_size, batch_size=batch_size, shuffle=False)

class_names = train_ds.class_names
num_classes = len(class_names)

# Step 5: Prefetch
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

# Step 6: Load EfficientNetB0 Base
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

# Step 7: Add Custom Layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Step 8: Compile Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Step 9: Train the Model
history = model.fit(train_ds, epochs=10, validation_data=val_ds)

# Step 10: Evaluate the Model
loss, accuracy = model.evaluate(test_ds)
print(f"✅ Test Accuracy: {accuracy*100:.2f}%")

# Step 11: Plot Accuracy and Loss
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Step 12: Confusion Matrix
# Get true labels and predictions
y_true = []
y_pred = []

for images, labels in test_ds:
    preds = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix on Test Data")
plt.show()

# Save the entire model
model.save('/content/drive/MyDrive/LDC/efficientnet_lung_model.h5')

# import os
# import matplotlib.pyplot as plt
# from tensorflow.keras.preprocessing import image
# from tensorflow.keras.applications.efficientnet import preprocess_input
# import numpy as np

# def predict_lung_diseases_in_folder(folder_path, model, class_names):
#     image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

#     if not image_files:
#         print("❌ No image files found in the folder.")
#         return

#     for img_name in image_files:
#         img_path = os.path.join(folder_path, img_name)
#         img = image.load_img(img_path, target_size=(224, 224))
#         # Convert the PIL Image to a NumPy array
#         img_array = image.img_to_array(img)
#         img_array_exp = np.expand_dims(img_array, axis=0)
#         img_array_exp = preprocess_input(img_array_exp)

#         preds = model.predict(img_array_exp)
#         predicted_class = class_names[np.argmax(preds)]
#         confidence = np.max(preds) * 100

#         # Display the image with prediction using the NumPy array
#         plt.imshow(img_array.astype('uint8'))
#         plt.title(f"{img_name}\nPredicted: {predicted_class} ({confidence:.2f}%)")
#         plt.axis('off')
#         plt.show()

# test_image_folder = '/content/drive/MyDrive/LDC/data/test/normal'
# predict_lung_diseases_in_folder(test_image_folder, model, class_names)

from google.colab import files
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.efficientnet import preprocess_input
import numpy as np
import matplotlib.pyplot as plt

def predict_lung_disease_from_uploaded_image(model, class_names):

    # Upload image
    uploaded = files.upload()
    for file_name in uploaded.keys():

        # Load and preprocess the image
        img_path = file_name
        img = image.load_img(img_path, target_size=(224, 224))
        img_array = image.img_to_array(img) # Convert PIL Image to NumPy array
        img_array_exp = np.expand_dims(img_array, axis=0)
        img_array_exp = preprocess_input(img_array_exp)

        # Make prediction
        preds = model.predict(img_array_exp)
        predicted_class = class_names[np.argmax(preds)]
        confidence = np.max(preds) * 100

        # Display result
        plt.imshow(img_array.astype('uint8')) # Use the NumPy array for display
        plt.title(f"Predicted: {predicted_class} ({confidence:.2f}%)")
        plt.axis('off')
        plt.show()

predict_lung_disease_from_uploaded_image(model, class_names)

